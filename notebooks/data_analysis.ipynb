{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "# NLTK\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# GENSIM\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# SKLEARN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tristan\\AppData\\Local\\Temp\\ipykernel_10044\\3499226996.py:1: DtypeWarning: Columns (7,8,9,10,11,24,60,98,99,101,102,104,105,107,108,110,111,113,114,116,117,119,120,122,123,125,126,128,129,131,132,134,135,137,138,140,141,143,144,146,147,149,150,152,153,155,156,158,159,161,162,164,165,167,168,170,171,173,174,176,177,179,180,182,183,185,186,188,189,191,192,194,195,197,198,200,201,203,204,206,207,209,210) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/dataset_twitter-scraper_2023-02-16_09-48-43-259.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/dataset_twitter-scraper_2023-02-16_09-48-43-259.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags/0</th>\n",
       "      <th>hashtags/1</th>\n",
       "      <th>hashtags/2</th>\n",
       "      <th>hashtags/3</th>\n",
       "      <th>hashtags/4</th>\n",
       "      <th>hashtags/5</th>\n",
       "      <th>...</th>\n",
       "      <th>user_mentions/47/id_str</th>\n",
       "      <th>user_mentions/47/name</th>\n",
       "      <th>user_mentions/47/screen_name</th>\n",
       "      <th>user_mentions/48/id_str</th>\n",
       "      <th>user_mentions/48/name</th>\n",
       "      <th>user_mentions/48/screen_name</th>\n",
       "      <th>user_mentions/49/id_str</th>\n",
       "      <th>user_mentions/49/name</th>\n",
       "      <th>user_mentions/49/screen_name</th>\n",
       "      <th>view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1625495400470958082</td>\n",
       "      <td>2023-02-14T14:01:18.000Z</td>\n",
       "      <td>410055</td>\n",
       "      <td>Happy Valentine’s Day to the one and only, @Mi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16532336.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1625143593286438912</td>\n",
       "      <td>2023-02-13T14:43:21.000Z</td>\n",
       "      <td>28215</td>\n",
       "      <td>Congratulations to the Kansas City @Chiefs, Pa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2775676.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1623489922438156288</td>\n",
       "      <td>2023-02-09T01:12:15.000Z</td>\n",
       "      <td>9131</td>\n",
       "      <td>@KingJames has been changing the game for 20 y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>492644.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1625903851168579585</td>\n",
       "      <td>2023-02-16T06:21:12.000Z</td>\n",
       "      <td>674</td>\n",
       "      <td>@RokoMijic !!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>443693.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1622770990710235136</td>\n",
       "      <td>2023-02-07T01:35:28.000Z</td>\n",
       "      <td>13821</td>\n",
       "      <td>The scale of devastation after the earthquakes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3117489.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 212 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       conversation_id                created_at  favorite_count  \\\n",
       "0  1625495400470958082  2023-02-14T14:01:18.000Z          410055   \n",
       "1  1625143593286438912  2023-02-13T14:43:21.000Z           28215   \n",
       "2  1623489922438156288  2023-02-09T01:12:15.000Z            9131   \n",
       "3  1625903851168579585  2023-02-16T06:21:12.000Z             674   \n",
       "4  1622770990710235136  2023-02-07T01:35:28.000Z           13821   \n",
       "\n",
       "                                           full_text hashtags/0 hashtags/1  \\\n",
       "0  Happy Valentine’s Day to the one and only, @Mi...        NaN        NaN   \n",
       "1  Congratulations to the Kansas City @Chiefs, Pa...        NaN        NaN   \n",
       "2  @KingJames has been changing the game for 20 y...        NaN        NaN   \n",
       "3                                      @RokoMijic !!        NaN        NaN   \n",
       "4  The scale of devastation after the earthquakes...        NaN        NaN   \n",
       "\n",
       "  hashtags/2 hashtags/3 hashtags/4 hashtags/5  ... user_mentions/47/id_str  \\\n",
       "0        NaN        NaN        NaN        NaN  ...                     NaN   \n",
       "1        NaN        NaN        NaN        NaN  ...                     NaN   \n",
       "2        NaN        NaN        NaN        NaN  ...                     NaN   \n",
       "3        NaN        NaN        NaN        NaN  ...                     NaN   \n",
       "4        NaN        NaN        NaN        NaN  ...                     NaN   \n",
       "\n",
       "  user_mentions/47/name  user_mentions/47/screen_name user_mentions/48/id_str  \\\n",
       "0                   NaN                           NaN                     NaN   \n",
       "1                   NaN                           NaN                     NaN   \n",
       "2                   NaN                           NaN                     NaN   \n",
       "3                   NaN                           NaN                     NaN   \n",
       "4                   NaN                           NaN                     NaN   \n",
       "\n",
       "  user_mentions/48/name user_mentions/48/screen_name user_mentions/49/id_str  \\\n",
       "0                   NaN                          NaN                     NaN   \n",
       "1                   NaN                          NaN                     NaN   \n",
       "2                   NaN                          NaN                     NaN   \n",
       "3                   NaN                          NaN                     NaN   \n",
       "4                   NaN                          NaN                     NaN   \n",
       "\n",
       "  user_mentions/49/name user_mentions/49/screen_name  view_count  \n",
       "0                   NaN                          NaN  16532336.0  \n",
       "1                   NaN                          NaN   2775676.0  \n",
       "2                   NaN                          NaN    492644.0  \n",
       "3                   NaN                          NaN    443693.0  \n",
       "4                   NaN                          NaN   3117489.0  \n",
       "\n",
       "[5 rows x 212 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing\n",
    "\n",
    "Here we define a function that will help us pre-process our data, this includes cleaning the text data, tokenizing, and removing stop words, punctuation and special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tristan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "custom_stopwords = {'\"', \"'\", \"rt\", \"’\", \"“\", \"”\", \"…\", \"‘\"}\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https\\S+', '', text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase \n",
    "    text = text.lower()\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english')) | custom_stopwords\n",
    "    text_tokens = word_tokenize(text)\n",
    "\n",
    "    filtered_text = [word for word in text_tokens if word not in stop_words]\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis\n",
    "\n",
    "The first feature we want to extract from the text is sentiment score. The sentiment score will help us determine the setiment of a user towards a particular topic or a group of people, this will be a particularily helpful feature for the next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(tokens):\n",
    "    text = ' '.join(tokens)\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the text data\n",
    "df['clean_text'] = df['full_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the sentiment score for each Tweet\n",
    "df['sentiment'] = df['clean_text'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags/0</th>\n",
       "      <th>hashtags/1</th>\n",
       "      <th>hashtags/2</th>\n",
       "      <th>hashtags/3</th>\n",
       "      <th>hashtags/4</th>\n",
       "      <th>hashtags/5</th>\n",
       "      <th>...</th>\n",
       "      <th>user_mentions/48/screen_name</th>\n",
       "      <th>user_mentions/49/id_str</th>\n",
       "      <th>user_mentions/49/name</th>\n",
       "      <th>user_mentions/49/screen_name</th>\n",
       "      <th>view_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>topic_distribution</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1625495400470958082</td>\n",
       "      <td>2023-02-14T14:01:18.000Z</td>\n",
       "      <td>410055</td>\n",
       "      <td>Happy Valentine’s Day to the one and only, @Mi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16532336.0</td>\n",
       "      <td>[happy, valentine, day, one, make, every, day,...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>[(3, 0.3180154), (18, 0.07663138), (43, 0.0995...</td>\n",
       "      <td>49.0</td>\n",
       "      <td>{3: 0.3179696, 18: 0.076635316, 43: 0.09959236...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1625143593286438912</td>\n",
       "      <td>2023-02-13T14:43:21.000Z</td>\n",
       "      <td>28215</td>\n",
       "      <td>Congratulations to the Kansas City @Chiefs, Pa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2775676.0</td>\n",
       "      <td>[congratulations, kansas, city, patrick, mahom...</td>\n",
       "      <td>0.526667</td>\n",
       "      <td>[(23, 0.037654456), (59, 0.8535993), (80, 0.05...</td>\n",
       "      <td>59.0</td>\n",
       "      <td>{23: 0.037670366, 59: 0.8535833, 80: 0.0554476}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1623489922438156288</td>\n",
       "      <td>2023-02-09T01:12:15.000Z</td>\n",
       "      <td>9131</td>\n",
       "      <td>@KingJames has been changing the game for 20 y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>492644.0</td>\n",
       "      <td>[changing, game, 20, years, become, leader, co...</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>[(1, 0.09189626), (8, 0.091813624), (42, 0.092...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>{1: 0.091896296, 8: 0.09181362, 42: 0.09238735...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1625903851168579585</td>\n",
       "      <td>2023-02-16T06:21:12.000Z</td>\n",
       "      <td>674</td>\n",
       "      <td>@RokoMijic !!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>443693.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1622770990710235136</td>\n",
       "      <td>2023-02-07T01:35:28.000Z</td>\n",
       "      <td>13821</td>\n",
       "      <td>The scale of devastation after the earthquakes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3117489.0</td>\n",
       "      <td>[scale, devastation, earthquakes, türkiye, syr...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[(24, 0.05606853), (65, 0.64960015), (70, 0.09...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>{24: 0.056062967, 65: 0.6494405, 70: 0.0932677...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       conversation_id                created_at  favorite_count  \\\n",
       "0  1625495400470958082  2023-02-14T14:01:18.000Z          410055   \n",
       "1  1625143593286438912  2023-02-13T14:43:21.000Z           28215   \n",
       "2  1623489922438156288  2023-02-09T01:12:15.000Z            9131   \n",
       "3  1625903851168579585  2023-02-16T06:21:12.000Z             674   \n",
       "4  1622770990710235136  2023-02-07T01:35:28.000Z           13821   \n",
       "\n",
       "                                           full_text hashtags/0 hashtags/1  \\\n",
       "0  Happy Valentine’s Day to the one and only, @Mi...        NaN        NaN   \n",
       "1  Congratulations to the Kansas City @Chiefs, Pa...        NaN        NaN   \n",
       "2  @KingJames has been changing the game for 20 y...        NaN        NaN   \n",
       "3                                      @RokoMijic !!        NaN        NaN   \n",
       "4  The scale of devastation after the earthquakes...        NaN        NaN   \n",
       "\n",
       "  hashtags/2 hashtags/3 hashtags/4 hashtags/5  ...  \\\n",
       "0        NaN        NaN        NaN        NaN  ...   \n",
       "1        NaN        NaN        NaN        NaN  ...   \n",
       "2        NaN        NaN        NaN        NaN  ...   \n",
       "3        NaN        NaN        NaN        NaN  ...   \n",
       "4        NaN        NaN        NaN        NaN  ...   \n",
       "\n",
       "  user_mentions/48/screen_name user_mentions/49/id_str  user_mentions/49/name  \\\n",
       "0                          NaN                     NaN                    NaN   \n",
       "1                          NaN                     NaN                    NaN   \n",
       "2                          NaN                     NaN                    NaN   \n",
       "3                          NaN                     NaN                    NaN   \n",
       "4                          NaN                     NaN                    NaN   \n",
       "\n",
       "  user_mentions/49/screen_name  view_count  \\\n",
       "0                          NaN  16532336.0   \n",
       "1                          NaN   2775676.0   \n",
       "2                          NaN    492644.0   \n",
       "3                          NaN    443693.0   \n",
       "4                          NaN   3117489.0   \n",
       "\n",
       "                                          clean_text sentiment  \\\n",
       "0  [happy, valentine, day, one, make, every, day,...  0.800000   \n",
       "1  [congratulations, kansas, city, patrick, mahom...  0.526667   \n",
       "2  [changing, game, 20, years, become, leader, co... -0.400000   \n",
       "3                                                 []  0.000000   \n",
       "4  [scale, devastation, earthquakes, türkiye, syr...  0.000000   \n",
       "\n",
       "                                  topic_distribution topic  \\\n",
       "0  [(3, 0.3180154), (18, 0.07663138), (43, 0.0995...  49.0   \n",
       "1  [(23, 0.037654456), (59, 0.8535993), (80, 0.05...  59.0   \n",
       "2  [(1, 0.09189626), (8, 0.091813624), (42, 0.092...  98.0   \n",
       "3                                                 []   NaN   \n",
       "4  [(24, 0.05606853), (65, 0.64960015), (70, 0.09...  65.0   \n",
       "\n",
       "                                 topic_probabilities  \n",
       "0  {3: 0.3179696, 18: 0.076635316, 43: 0.09959236...  \n",
       "1    {23: 0.037670366, 59: 0.8535833, 80: 0.0554476}  \n",
       "2  {1: 0.091896296, 8: 0.09181362, 42: 0.09238735...  \n",
       "3                                               None  \n",
       "4  {24: 0.056062967, 65: 0.6494405, 70: 0.0932677...  \n",
       "\n",
       "[5 rows x 217 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic modeling\n",
    "\n",
    "Topic modeling will allow us to indentify topics or themes in our corpus. Combined with the setiment analysis, we should be able to determine which topics the tweet is negative or psotive about.\n",
    "Since our text is already pre-processed, there is no need to process it any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing all the processed text in a variable\n",
    "processed_texts = df['clean_text']\n",
    "\n",
    "# Create a dictionary of terms\n",
    "dictionary = Dictionary(processed_texts)\n",
    "\n",
    "# Create a corpus of documents\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the LDA model\n",
    "lda_model = LdaModel(corpus= corpus, num_topics= 89, id2word= dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 54: stream censorship tweets concerns accountability normal decided rules trouble blame\n",
      "Topic 28: believe biden enough doesnt chinese stay americans worth matter watching\n",
      "Topic 71: even important one anything wont loved fully i… opinion ones\n",
      "Topic 74: amp book woke system shows heard today clearly wish brand\n",
      "Topic 88: idea arrested tech race hey big criminal general electric seem\n",
      "Topic 23: going oh isnt right andrew tate food hold id price\n",
      "Topic 27: see future taking hes could greatest rest link stupid industry\n",
      "Topic 24: stop 10 republicans try security policy held brought cut helping\n",
      "Topic 17: days sign set level w despite 26 prince poor success\n",
      "Topic 81: public two called gender school information completely example governor private\n"
     ]
    }
   ],
   "source": [
    "for i, topic in lda_model.show_topics(num_topics=10, num_words=10, formatted=False):\n",
    "    print('Topic {}: {}'.format(i, ' '.join([w[0] for w in topic])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  -34.02880123662646\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity\n",
    "print('Perplexity: ', lda_model.log_perplexity(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence:  0.4727003262469014\n"
     ]
    }
   ],
   "source": [
    "# Calculate coherence\n",
    "coherence_model_lda = CoherenceModel(model= lda_model, texts= processed_texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence: ', coherence_lda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our LDA model is trained, we will create a new column in our dataframe to store the topic distribution for each Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the topic distributions for each document\n",
    "doc_topic_dists = lda_model.get_document_topics(corpus)\n",
    "\n",
    "# create a new column in the dataframe to store the topic distribution for each document\n",
    "df['topic_distribution'] = doc_topic_dists\n",
    "\n",
    "# extract the most likely topic for each document\n",
    "df['topic'] = [max(doc, key=lambda item: item[1])[0] if doc else None for doc in doc_topic_dists]\n",
    "\n",
    "# extract the topic probabilities for each document\n",
    "df['topic_probabilities'] = [dict(doc) if doc else None for doc in doc_topic_dists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags/0</th>\n",
       "      <th>hashtags/1</th>\n",
       "      <th>hashtags/2</th>\n",
       "      <th>hashtags/3</th>\n",
       "      <th>hashtags/4</th>\n",
       "      <th>hashtags/5</th>\n",
       "      <th>...</th>\n",
       "      <th>user_mentions/48/screen_name</th>\n",
       "      <th>user_mentions/49/id_str</th>\n",
       "      <th>user_mentions/49/name</th>\n",
       "      <th>user_mentions/49/screen_name</th>\n",
       "      <th>view_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>topic_distribution</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1625495400470958082</td>\n",
       "      <td>2023-02-14T14:01:18.000Z</td>\n",
       "      <td>410055</td>\n",
       "      <td>Happy Valentine’s Day to the one and only, @Mi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16532336.0</td>\n",
       "      <td>[happy, valentine, day, one, make, every, day,...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>[(77, 0.7816087), (80, 0.12063725)]</td>\n",
       "      <td>77</td>\n",
       "      <td>{77: 0.77987033, 80: 0.12237568}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1625143593286438912</td>\n",
       "      <td>2023-02-13T14:43:21.000Z</td>\n",
       "      <td>28215</td>\n",
       "      <td>Congratulations to the Kansas City @Chiefs, Pa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2775676.0</td>\n",
       "      <td>[congratulations, kansas, city, patrick, mahom...</td>\n",
       "      <td>0.526667</td>\n",
       "      <td>[(3, 0.110720865), (26, 0.05747989), (27, 0.05...</td>\n",
       "      <td>82</td>\n",
       "      <td>{3: 0.11072106, 26: 0.057477057, 27: 0.0538410...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1623489922438156288</td>\n",
       "      <td>2023-02-09T01:12:15.000Z</td>\n",
       "      <td>9131</td>\n",
       "      <td>@KingJames has been changing the game for 20 y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>492644.0</td>\n",
       "      <td>[changing, game, 20, years, become, leader, co...</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>[(10, 0.069983415), (26, 0.10043924), (40, 0.0...</td>\n",
       "      <td>80</td>\n",
       "      <td>{10: 0.069958955, 26: 0.10045136, 40: 0.091927...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1625903851168579585</td>\n",
       "      <td>2023-02-16T06:21:12.000Z</td>\n",
       "      <td>674</td>\n",
       "      <td>@RokoMijic !!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>443693.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[(0, 0.011235955), (1, 0.011235955), (2, 0.011...</td>\n",
       "      <td>0</td>\n",
       "      <td>{0: 0.011235955, 1: 0.011235955, 2: 0.01123595...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1622770990710235136</td>\n",
       "      <td>2023-02-07T01:35:28.000Z</td>\n",
       "      <td>13821</td>\n",
       "      <td>The scale of devastation after the earthquakes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3117489.0</td>\n",
       "      <td>[scale, devastation, earthquakes, türkiye, syr...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[(1, 0.054576986), (15, 0.051483113), (39, 0.5...</td>\n",
       "      <td>39</td>\n",
       "      <td>{1: 0.05456632, 15: 0.051481012, 39: 0.5493744...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1625725531336638464</td>\n",
       "      <td>2023-02-16T06:14:04.000Z</td>\n",
       "      <td>1319</td>\n",
       "      <td>@BillyM2k @cb_doge Well I would hope that most...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83061.0</td>\n",
       "      <td>[well, would, hope, san, franciscans, agree, p...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[(22, 0.15038377), (30, 0.14446957), (31, 0.12...</td>\n",
       "      <td>57</td>\n",
       "      <td>{22: 0.15049213, 30: 0.14446954, 31: 0.1288767...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1620905250742730756</td>\n",
       "      <td>2023-02-01T22:01:41.000Z</td>\n",
       "      <td>12351</td>\n",
       "      <td>At my last Black History Month celebration at ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1133433.0</td>\n",
       "      <td>[last, black, history, month, celebration, whi...</td>\n",
       "      <td>-0.013333</td>\n",
       "      <td>[(5, 0.04804414), (12, 0.093949236), (13, 0.06...</td>\n",
       "      <td>46</td>\n",
       "      <td>{5: 0.048043806, 12: 0.09394804, 13: 0.0655410...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1620905250742730756</td>\n",
       "      <td>2023-02-01T22:01:41.000Z</td>\n",
       "      <td>45433</td>\n",
       "      <td>Black History Month is about the shared experi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3302865.0</td>\n",
       "      <td>[black, history, month, shared, experience, bl...</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>[(12, 0.15354958), (26, 0.07779298), (28, 0.07...</td>\n",
       "      <td>60</td>\n",
       "      <td>{12: 0.15354933, 26: 0.077792995, 28: 0.077791...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1626097497109311495</td>\n",
       "      <td>2023-02-16T06:07:30.000Z</td>\n",
       "      <td>633</td>\n",
       "      <td>@wongmjane Yeah, it would be *crazy* to make a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82777.0</td>\n",
       "      <td>[yeah, would, crazy, make, ai, like, irl]</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>[(31, 0.4054175), (44, 0.14446111), (47, 0.168...</td>\n",
       "      <td>31</td>\n",
       "      <td>{31: 0.40514112, 44: 0.1444611, 47: 0.16917141...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1626097497109311495</td>\n",
       "      <td>2023-02-16T05:57:38.000Z</td>\n",
       "      <td>11043</td>\n",
       "      <td>Sounds eerily like the AI in System Shock that...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1845927.0</td>\n",
       "      <td>[sounds, eerily, like, ai, system, shock, goes...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[(13, 0.3481298), (19, 0.1588142), (53, 0.1016...</td>\n",
       "      <td>13</td>\n",
       "      <td>{13: 0.34824863, 19: 0.1587027, 53: 0.10160482...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       conversation_id                created_at  favorite_count  \\\n",
       "0  1625495400470958082  2023-02-14T14:01:18.000Z          410055   \n",
       "1  1625143593286438912  2023-02-13T14:43:21.000Z           28215   \n",
       "2  1623489922438156288  2023-02-09T01:12:15.000Z            9131   \n",
       "3  1625903851168579585  2023-02-16T06:21:12.000Z             674   \n",
       "4  1622770990710235136  2023-02-07T01:35:28.000Z           13821   \n",
       "5  1625725531336638464  2023-02-16T06:14:04.000Z            1319   \n",
       "6  1620905250742730756  2023-02-01T22:01:41.000Z           12351   \n",
       "7  1620905250742730756  2023-02-01T22:01:41.000Z           45433   \n",
       "8  1626097497109311495  2023-02-16T06:07:30.000Z             633   \n",
       "9  1626097497109311495  2023-02-16T05:57:38.000Z           11043   \n",
       "\n",
       "                                           full_text hashtags/0 hashtags/1  \\\n",
       "0  Happy Valentine’s Day to the one and only, @Mi...        NaN        NaN   \n",
       "1  Congratulations to the Kansas City @Chiefs, Pa...        NaN        NaN   \n",
       "2  @KingJames has been changing the game for 20 y...        NaN        NaN   \n",
       "3                                      @RokoMijic !!        NaN        NaN   \n",
       "4  The scale of devastation after the earthquakes...        NaN        NaN   \n",
       "5  @BillyM2k @cb_doge Well I would hope that most...        NaN        NaN   \n",
       "6  At my last Black History Month celebration at ...        NaN        NaN   \n",
       "7  Black History Month is about the shared experi...        NaN        NaN   \n",
       "8  @wongmjane Yeah, it would be *crazy* to make a...        NaN        NaN   \n",
       "9  Sounds eerily like the AI in System Shock that...        NaN        NaN   \n",
       "\n",
       "  hashtags/2 hashtags/3 hashtags/4 hashtags/5  ...  \\\n",
       "0        NaN        NaN        NaN        NaN  ...   \n",
       "1        NaN        NaN        NaN        NaN  ...   \n",
       "2        NaN        NaN        NaN        NaN  ...   \n",
       "3        NaN        NaN        NaN        NaN  ...   \n",
       "4        NaN        NaN        NaN        NaN  ...   \n",
       "5        NaN        NaN        NaN        NaN  ...   \n",
       "6        NaN        NaN        NaN        NaN  ...   \n",
       "7        NaN        NaN        NaN        NaN  ...   \n",
       "8        NaN        NaN        NaN        NaN  ...   \n",
       "9        NaN        NaN        NaN        NaN  ...   \n",
       "\n",
       "  user_mentions/48/screen_name user_mentions/49/id_str  user_mentions/49/name  \\\n",
       "0                          NaN                     NaN                    NaN   \n",
       "1                          NaN                     NaN                    NaN   \n",
       "2                          NaN                     NaN                    NaN   \n",
       "3                          NaN                     NaN                    NaN   \n",
       "4                          NaN                     NaN                    NaN   \n",
       "5                          NaN                     NaN                    NaN   \n",
       "6                          NaN                     NaN                    NaN   \n",
       "7                          NaN                     NaN                    NaN   \n",
       "8                          NaN                     NaN                    NaN   \n",
       "9                          NaN                     NaN                    NaN   \n",
       "\n",
       "  user_mentions/49/screen_name  view_count  \\\n",
       "0                          NaN  16532336.0   \n",
       "1                          NaN   2775676.0   \n",
       "2                          NaN    492644.0   \n",
       "3                          NaN    443693.0   \n",
       "4                          NaN   3117489.0   \n",
       "5                          NaN     83061.0   \n",
       "6                          NaN   1133433.0   \n",
       "7                          NaN   3302865.0   \n",
       "8                          NaN     82777.0   \n",
       "9                          NaN   1845927.0   \n",
       "\n",
       "                                          clean_text sentiment  \\\n",
       "0  [happy, valentine, day, one, make, every, day,...  0.800000   \n",
       "1  [congratulations, kansas, city, patrick, mahom...  0.526667   \n",
       "2  [changing, game, 20, years, become, leader, co... -0.400000   \n",
       "3                                                 []  0.000000   \n",
       "4  [scale, devastation, earthquakes, türkiye, syr...  0.000000   \n",
       "5  [well, would, hope, san, franciscans, agree, p...  0.000000   \n",
       "6  [last, black, history, month, celebration, whi... -0.013333   \n",
       "7  [black, history, month, shared, experience, bl... -0.111111   \n",
       "8          [yeah, would, crazy, make, ai, like, irl] -0.600000   \n",
       "9  [sounds, eerily, like, ai, system, shock, goes...  0.000000   \n",
       "\n",
       "                                  topic_distribution topic  \\\n",
       "0                [(77, 0.7816087), (80, 0.12063725)]    77   \n",
       "1  [(3, 0.110720865), (26, 0.05747989), (27, 0.05...    82   \n",
       "2  [(10, 0.069983415), (26, 0.10043924), (40, 0.0...    80   \n",
       "3  [(0, 0.011235955), (1, 0.011235955), (2, 0.011...     0   \n",
       "4  [(1, 0.054576986), (15, 0.051483113), (39, 0.5...    39   \n",
       "5  [(22, 0.15038377), (30, 0.14446957), (31, 0.12...    57   \n",
       "6  [(5, 0.04804414), (12, 0.093949236), (13, 0.06...    46   \n",
       "7  [(12, 0.15354958), (26, 0.07779298), (28, 0.07...    60   \n",
       "8  [(31, 0.4054175), (44, 0.14446111), (47, 0.168...    31   \n",
       "9  [(13, 0.3481298), (19, 0.1588142), (53, 0.1016...    13   \n",
       "\n",
       "                                 topic_probabilities  \n",
       "0                   {77: 0.77987033, 80: 0.12237568}  \n",
       "1  {3: 0.11072106, 26: 0.057477057, 27: 0.0538410...  \n",
       "2  {10: 0.069958955, 26: 0.10045136, 40: 0.091927...  \n",
       "3  {0: 0.011235955, 1: 0.011235955, 2: 0.01123595...  \n",
       "4  {1: 0.05456632, 15: 0.051481012, 39: 0.5493744...  \n",
       "5  {22: 0.15049213, 30: 0.14446954, 31: 0.1288767...  \n",
       "6  {5: 0.048043806, 12: 0.09394804, 13: 0.0655410...  \n",
       "7  {12: 0.15354933, 26: 0.077792995, 28: 0.077791...  \n",
       "8  {31: 0.40514112, 44: 0.1444611, 47: 0.16917141...  \n",
       "9  {13: 0.34824863, 19: 0.1587027, 53: 0.10160482...  \n",
       "\n",
       "[10 rows x 217 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coherence_perplexity(texts, corpus, dictionary, start=2, limit=100, step=1):\n",
    "    coherence_scores = []\n",
    "    perplexity_scores = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        lda_model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_score = coherence_model_lda.get_coherence()\n",
    "        coherence_scores.append(coherence_score)\n",
    "        perplexity_scores.append(lda_model.log_perplexity(corpus))\n",
    "        print('Number of Topics:', num_topics, '  Coherence Score:', coherence_score, '  Perplexity Score:', lda_model.log_perplexity(corpus))\n",
    "    \n",
    "    # plot the coherence and perplexity scores\n",
    "    x = range(start, limit, step)\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(x, coherence_scores, color='blue')\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('Coherence Score', color='blue')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, perplexity_scores, color='red')\n",
    "    ax2.set_ylabel('Perplexity Score', color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3378, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Temp\\ipykernel_10044\\448380629.py\", line 1, in <module>\n",
      "    plot_coherence_perplexity(processed_texts, corpus, dictionary)\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Temp\\ipykernel_10044\\3499039388.py\", line 5, in plot_coherence_perplexity\n",
      "    lda_model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gensim\\models\\ldamodel.py\", line 521, in __init__\n",
      "    self.update(corpus, chunks_as_numpy=use_numpy)\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gensim\\models\\ldamodel.py\", line 1006, in update\n",
      "    gammat = self.do_estep(chunk, other)\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gensim\\models\\ldamodel.py\", line 768, in do_estep\n",
      "    gamma, sstats = self.inference(chunk, collect_sstats=True)\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gensim\\models\\ldamodel.py\", line -1, in inference\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 1997, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1112, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1006, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 859, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 812, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 730, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\Tristan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "plot_coherence_perplexity(processed_texts, corpus, dictionary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "89 n topics looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a string representation of a topic distribution to a NumPy array\n",
    "def parse_topic_distribution(topic_dist_str):\n",
    "    topic_probs = np.zeros(100)  # Assumes there are 100 topics\n",
    "    for topic, prob in eval(topic_dist_str):\n",
    "        topic_probs[topic] = prob\n",
    "    return topic_probs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "\n",
    "Now that we have processed the data to produce relevant features, we want to train a machine learning algorithm using those features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "eval() arg 1 must be a string, bytes or code object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [110], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtopic_distribution\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mtopic_distribution\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[39m# Create a new column in the dataframe with the topic distributions as NumPy arrays\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtopic_probs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mtopic_distribution\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(parse_topic_distribution)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\series.py:4774\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4664\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4665\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4666\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4669\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4670\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4671\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4672\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4673\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4772\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4773\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4774\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\apply.py:1100\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1097\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1099\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1100\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\apply.py:1151\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1150\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1151\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1152\u001b[0m             values,\n\u001b[0;32m   1153\u001b[0m             f,\n\u001b[0;32m   1154\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1155\u001b[0m         )\n\u001b[0;32m   1157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1158\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1159\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\_libs\\lib.pyx:2919\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn [106], line 4\u001b[0m, in \u001b[0;36mparse_topic_distribution\u001b[1;34m(topic_dist_str)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_topic_distribution\u001b[39m(topic_dist_str):\n\u001b[0;32m      3\u001b[0m     topic_probs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39m100\u001b[39m)  \u001b[39m# Assumes there are 100 topics\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mfor\u001b[39;00m topic, prob \u001b[39min\u001b[39;00m \u001b[39meval\u001b[39;49m(topic_dist_str):\n\u001b[0;32m      5\u001b[0m         topic_probs[topic] \u001b[39m=\u001b[39m prob\n\u001b[0;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m topic_probs\n",
      "\u001b[1;31mTypeError\u001b[0m: eval() arg 1 must be a string, bytes or code object"
     ]
    }
   ],
   "source": [
    "df['topic_distribution'] = df['topic_distribution'].astype('object')\n",
    "# Create a new column in the dataframe with the topic distributions as NumPy arrays\n",
    "df['topic_probs'] = df['topic_distribution'].apply(parse_topic_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [104], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m# Define and fit the SVM model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m clf \u001b[39m=\u001b[39m SVC(kernel\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     12\u001b[0m \u001b[39m# Make predictions on the testing data and evaluate the performance of the model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_base.py:173\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    171\u001b[0m     check_consistent_length(X, y)\n\u001b[0;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 173\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    174\u001b[0m         X,\n\u001b[0;32m    175\u001b[0m         y,\n\u001b[0;32m    176\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[0;32m    177\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    178\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    179\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    180\u001b[0m     )\n\u001b[0;32m    182\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_targets(y)\n\u001b[0;32m    184\u001b[0m sample_weight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\n\u001b[0;32m    185\u001b[0m     [] \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m sample_weight, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64\n\u001b[0;32m    186\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    594\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    595\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 596\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1074\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1069\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1070\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1071\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1072\u001b[0m     )\n\u001b[1;32m-> 1074\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1075\u001b[0m     X,\n\u001b[0;32m   1076\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1077\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1078\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1079\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1080\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1081\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1082\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1083\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1084\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1085\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1086\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1087\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1088\u001b[0m )\n\u001b[0;32m   1090\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1092\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    854\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    855\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 856\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    857\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    858\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    860\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\generic.py:2069\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2068\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m-> 2069\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Define the features and target variable\n",
    "X = df[['topic_distribution', 'sentiment']]\n",
    "y = df['topic']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define and fit the SVM model\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data and evaluate the performance of the model\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2be8123af0d534fc0aaa90e23b17b8d369889f952e75ca4ecb84af4ee3f79746"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
